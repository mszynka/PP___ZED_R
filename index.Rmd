---
title: "Analiza danych krystalograficznych"
output:
  html_document:
    toc: true
    toc_float: true
---
Wygenerowano: `r format(Sys.time(), '%d %B, %Y')`

```{r hidden, echo=FALSE}
library(DT)
prettyTable <- function(table_df, round_columns=numeric(), round_digits=2) {
    DT::datatable(table_df, style="bootstrap", filter = "top", rownames = FALSE, extensions = "Buttons", options = list(dom = 'Bfrtip', buttons = c('copy', 'csv', 'excel', 'pdf', 'print'))) %>%
    formatRound(round_columns, round_digits)
}
```

#1. Wstęp

Chemicy i fizycy wykorzystują krystalografię jako jedną z najdokładniejszych metod określania struktury i budowy cząsteczek. Naświetlane promieniami rentgenowskimi kryształy odbijają uderzające w nie promienie, które są nagrywane przez aparat cyfrowy. Obraz z aparatu, po wzięciu pod uwagę interferencji i innych zjawisk zachodzących podczas przechodzenia promieni przez kryształy, stanowi podstawę do stworzenia map intensywności, czyli obrazów 3D określających prawdopodobieństwo występowania pierwiastków w przestrzeni. Takie mapy są następnie analizowane przez chemików, fizyków i biologów, którzy określają typy pierwiastków i połączenia między nimi.

Od pewnego czasu krystalografia pozwala określać budowę nie tylko małych cząsteczek, ale także większych struktur takich jak białka. Przy strukturach tak dużych i skomplikowanych jak białka, ręczne określenie wszystkich pierwiastków jest pracą długą i podatną na błędy. Stąd potrzeba rozwijania metod automatycznej analizy fragmentów map intensywności.

Celem projektu jest analiza danych krystalograficznych i ocena możliwości wykorzystania uczenia maszynowego do automatycznego podpowiadania biologom jakie cząsteczki mogą się kryć w niewymodelowanych fragmentach map intensywności.

#1.1 Opis danych

Dane do realizacji analizy pochodzą z bazy Protein Data Bank (PDB), gdzie przechowywane są dane strukturalne białek oraz innych związków chemicznych. Punktem zainteresowania tej analizy są małe cząsteczki (ligandy), które potrafią wiązać się z niektórymi białkami. Struktury zgromadzone w bazie PDB zostały pobrane i przetworzane, aby wybrać informacje tylko o ligandach i zakodować je w postaci kolumn opisanych poniżej. Opis każdego liganda oparty jest o trójwymiarowy fragment gęstości elektronowej struktury (tzw. maskę), czyli w uproszczeniu obraz 3D. Z jednego wpisu do bazy PDB może powstać kilka wierszy opisujących zawarte w strukturze ligandy. 

#2. Specyfikacja

Poniższa specyfikacja jest specyfikacją techniczną. W kolejnym punkcie prezentowane są wyniki analiz.

#2.1. Powtarzalność wyników

Zakładając identyczny plik wejściowy, powtarzalność wyników zapewniają konfiguracje ziarna wartości losowych np. `set.seed(x)`.

#2.2. Użyte biblioteki

W poniższej analizie zostały wykorzystane następujące biblioteki:
- knitr - do generowania raportu z R+markdown
- dplyr - do przetwarzania danych
- ggplot2 - do generowania wykresów
- reshape2 - do wstępnego przetwarzania danych
- plotly - do interaktywnych wykresów
- DT - do wyświetlania tabel danych

#3. Analiza danych

Poniższe analizy są realizowane zgodnie z poszczególnymi punktami założeń projektu.

#3.1. ładowanie danych

```{r readData, cache=TRUE, message=FALSE, warning=FALSE}
d <- read.csv(file="all_summary.csv", nrows=20000, header=TRUE, sep=";")
```

#3.2. Usuwanie wierszy o podanej klasie

```{r subsetWithList}
forbiddenValues <- list("UNK", "UNX", "UNL", "DUM", "N", "BLOB", "ALA", "ARG", "ASN", "ASP", "CYS", "GLN", "GLU", "GLY", "HIS", "ILE", "LEU", "LYS", "MET", "MSE", "PHE", "PRO", "SEC", "SER", "THR", "TRP", "TYR", "VAL", "DA", "DG", "DT", "DC", "DU", "A", "G", "T", "C", "U", "HOH", "H20", "WAT","UNK", "UNX", "UNL", "DUM", "N", "BLOB", "ALA", "ARG", "ASN", "ASP", "CYS", "GLN", "GLU", "GLY", "HIS", "ILE", "LEU", "LYS", "MET", "MSE", "PHE", "PRO", "SEC", "SER", "THR", "TRP", "TYR", "VAL", "DA", "DG", "DT", "DC", "DU", "A", "G", "T", "C", "U", "HOH", "H20", "WAT")

clean <- subset(d, !(res_name %in% forbiddenValues))
rm(forbiddenValues)
```

#3.3. Uzupełnienie brakujących danych

```{r fillMissing, warning=FALSE, cache=TRUE, paged.print=FALSE}
library(knitr)
# liczba kolumn, w których brakuje danych
length(colnames(clean)[ apply(clean, 2, anyNA) ])

getMostCommon <- function(x) { 
        names(which.max(table(x)))
}

# predykcja brakujących danych
filled <- apply(clean[,colnames(clean) %in% colnames(clean)[ apply(clean, 2, anyNA) ]],
      2,
      getMostCommon)
dim(clean)
rm(filled)
```

#3.4. Podstawowe statystyki

```{r, showDimensionsAndSummary, cache=TRUE}
library(knitr)
prettyTable(summary(clean))
```

#3.5. Najpopularniejsze ligandy

Poniższy kod ograniczaja liczbę klas (res_name) do 50 najpopularniejszych wartości i określa ile przykładów ma każda z klas (res_name).

```{r popularResNames, cache=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
ggplot(clean %>%
  group_by(res_name) %>%
  summarise(cnt=n()) %>%
  top_n(50), aes(x=res_name, y=cnt)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

#3.6. Korelacja pomiędzy wartościami

Sekcja sprawdzaja korelacje między zmiennymi. Wykres jest przedstawiony w formie interaktywnej z możliwością odczytania konkretnych wartości dzięki bibliotece plotly.

```{r varCorrelation, message=FALSE, warning=FALSE, cache=TRUE}
library(dplyr)
library(reshape2)
library(ggplot2)
library(plotly)

# ogranicz heatmap do macierzy trójkątnej
get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
}

#sortuj po wartościach
reorder_cormat <- function(cormat){
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <-cormat[hc$order, hc$order]
}

correlation <- cor(select_if(clean[,8:200], function(x){is.numeric(x) && !is.na(x)}))
ggplotly(ggplot(data=melt(get_upper_tri(correlation), na.rm = TRUE), aes(x=Var2, y=Var1, fill=value))+
  geom_tile()+ 
  theme(axis.text.x=element_text(size=4, angle=90))+
  theme(axis.text.y=element_text(size=4)))

# czyszczenie pamięci
rm(correlation)
```

#3.7. Wykresy rozkładów liczby atomów i elektronów.

Poniższy wykres przedstawia rozkład liczby atomów (local_res_atom_non_h_count) i elektronów (local_res_atom_non_h_electron_sum) względem siebie.

```{r atomCorrelation, message=FALSE, warning=FALSE, cache=TRUE}
library(ggplot2)
library(plotly)

ggplotly(ggplot(data=clean, aes(x=local_res_atom_non_h_count, y=local_res_atom_non_h_electron_sum))+
  geom_point()+
  theme_bw())
```

#3.8. Największe niezgodności liczby atomów i elektronów

Tabela pokazuje 10 klas z największą niezgodnością liczby atomów (local_res_atom_non_h_count vs dict_atom_non_h_count) i 10 klas z największą niezgodnością liczby elektronów (local_res_atom_non_h_electron_sum vs dict_atom_non_h_electron_sum) względem ich przewidywanych wartości słownikowych.

```{r atomDiffCorrelation, message=FALSE, warning=FALSE, cache=TRUE}
library(ggplot2)
library(plotly)
library(dplyr)

ggplotly(ggplot(data=clean %>%
  select(res_name, local_res_atom_non_h_count, dict_atom_non_h_count) %>%
  mutate(diff = abs(local_res_atom_non_h_count-dict_atom_non_h_count)) %>%
  top_n(10), aes(x=res_name, y=diff))+
  geom_point()+
  theme_bw())

ggplotly(ggplot(data=clean %>%
  select(res_name, local_res_atom_non_h_electron_sum, dict_atom_non_h_electron_sum) %>%
  mutate(diff = abs(local_res_atom_non_h_electron_sum-dict_atom_non_h_electron_sum)) %>%
  top_n(10), aes(x=res_name, y=diff))+
  geom_point()+
  theme_bw())
```

#3.9. Rozkład minimalnego progu maskowania

Sekcja pokazuje rozkład wartości wszystkich kolumn zaczynających się od part_01 z zaznaczeniem średniej wartości.

```{r part_01Distribution, message=FALSE, warning=FALSE, cache=TRUE}
library(ggplot2)
library(plotly)
library(reshape2)
library(dplyr)

data <- clean[,grepl("part_01", colnames(clean))] %>% 
         as.data.frame()

prettyTable(data)

rm(data)
```

#4. Wnioski

Projekt ten udowodnił, że R jest dobrym językiem do przetwarzania danych lecz jego czytelność i wsparcie jest znacząco niższe niż w przypadku Pythona. Nie mniej jednak szybkie efekty w połączeniu z interaktywnością i szybkością spowodowaną ograniczeniem pamięci operacyjnej do RAMu powodują, że jest to ciekawy język do zadań statystycznych.